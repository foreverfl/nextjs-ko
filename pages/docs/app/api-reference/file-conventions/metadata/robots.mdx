---
title: robots.txt
description: robots.txt 파일에 대한 API 참조.
---

# robots.txt

검색 엔진 크롤러에게 사이트의 어떤 URL에 접근할 수 있는지 알려주기 위해 [robots.txt 표준](https://en.wikipedia.org/wiki/Robots.txt#Standard)에 맞는 `robots.txt` 파일을 `app` 디렉토리의 **루트**에 추가하거나 생성합니다.

## 정적 `robots.txt`

```txt filename="app/robots.txt"
User-Agent: *
Allow: /
Disallow: /private/

Sitemap: https://acme.com/sitemap.xml
```

## Robots 파일 생성하기

[`Robots` 객체](#robots-object)를 반환하는 `robots.js` 또는 `robots.ts` 파일을 추가합니다.

> **알아두면 좋은 점**: `robots.js`는 [동적 함수](/docs/app/building-your-application/caching#dynamic-functions)나 [동적 config](/docs/app/building-your-application/caching#segment-config-options) 옵션을 사용하지 않는 한 기본적으로 캐시되는 특별한 Route Handler입니다.

```ts filename="app/robots.ts" switcher
import type { MetadataRoute } from "next";

export default function robots(): MetadataRoute.Robots {
  return {
    rules: {
      userAgent: "*",
      allow: "/",
      disallow: "/private/",
    },
    sitemap: "https://acme.com/sitemap.xml",
  };
}
```

```js filename="app/robots.js" switcher
export default function robots() {
  return {
    rules: {
      userAgent: "*",
      allow: "/",
      disallow: "/private/",
    },
    sitemap: "https://acme.com/sitemap.xml",
  };
}
```

출력:

```txt
User-Agent: *
Allow: /
Disallow: /private/

Sitemap: https://acme.com/sitemap.xml
```

### 특정 사용자 에이전트 커스터마이징

`rules` 속성에 사용자 에이전트 배열을 전달하여 개별 검색 엔진 봇이 사이트를 크롤하는 방식을 사용자 정의할 수 있습니다. 예를 들어:

```ts filename="app/robots.ts" switcher
import type { MetadataRoute } from "next";

export default function robots(): MetadataRoute.Robots {
  return {
    rules: [
      {
        userAgent: "Googlebot",
        allow: ["/"],
        disallow: "/private/",
      },
      {
        userAgent: ["Applebot", "Bingbot"],
        disallow: ["/"],
      },
    ],
    sitemap: "https://acme.com/sitemap.xml",
  };
}
```

```js filename="app/robots.js" switcher
export default function robots() {
  return {
    rules: [
      {
        userAgent: "Googlebot",
        allow: ["/"],
        disallow: ["/private/"],
      },
      {
        userAgent: ["Applebot", "Bingbot"],
        disallow: ["/"],
      },
    ],
    sitemap: "https://acme.com/sitemap.xml",
  };
}
```

출력:

```txt
User-Agent: Googlebot
Allow: /
Disallow: /private/

User-Agent: Applebot
Disallow: /

User-Agent: Bingbot
Disallow: /

Sitemap: https://acme.com/sitemap.xml
```

### Robots 객체

```tsx
type Robots = {
  rules:
    | {
        userAgent?: string | string[];
        allow?: string | string[];
        disallow?: string | string[];
        crawlDelay?: number;
      }
    | Array<{
        userAgent: string | string[];
        allow?: string | string[];
        disallow?: string | string[];
        crawlDelay?: number;
      }>;
  sitemap?: string | string[];
  host?: string;
};
```

## 버전 기록

| 버전      | 변경 사항      |
| --------- | -------------- |
| `v13.3.0` | `robots` 도입. |
